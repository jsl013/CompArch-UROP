{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "rotary-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "utility-asbestos",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "instructional-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layer,\n",
    "                 p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        #self.hidden = self.initHidden()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length(1개 input당), N(batch_size), input_size)\n",
    "        \n",
    "        embedding = self.embedding(x)\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "        encoder_states, (hidden, cell) = self.rnn(embedding)\n",
    "        # If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.\n",
    "        # hidden = h_n, cell = c_n (n=seq_length)\n",
    "        # encoder_state: (seq_length, batch_size, hidden_size)\n",
    "        \n",
    "        return encoder_states, hidden, cell\n",
    "        # encoder_states(output=hidden state for all time step): (seq_len, batch, num_directions * hidden_size)\n",
    "    #def initHidden(self):\n",
    "    #    return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "    \n",
    "    # Summerize\n",
    "    # LSTM init. : (input_size, hidden_size, num_layer)\n",
    "    # input : (seq_length, batch_size, input_size)\n",
    "    # hidden (of t) : (num_layer, batch_size, hidden_size)\n",
    "    # cell (of t) : (num_layer, batch_size,  hidden_size)\n",
    "    # output : (seq_length, batch_size, hidden_length)\n",
    "    # by FC layer, hidden_length --> output_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "unlimited-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size,\n",
    "                 num_layers, p): # how to choose output_size?\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(hidden_size + embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        \n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size, 1) # binary(0 or 1) classification\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, encoder_states, hidden, cell, scaling_factor):\n",
    "        # shape of x: (N) but we want (1, N)\n",
    "        x = x.unsqueeze(1)\n",
    "        # x: (1, N, input_size)\n",
    "        \n",
    "        embedding = self.embedding(x)\n",
    "        #embedding shape: (1, N, embedding_size)\n",
    "        \n",
    "        sequence_length = encoder_states.shape[0]\n",
    "        \n",
    "        # h: (1, batch_size, hidden_size) \n",
    "        h_reshaped = hidden\n",
    "        h_reshaped = h_reshaped.squeeze(0)\n",
    "        h_reshaped = h_reshaped.unsqueeze(1)\n",
    "        # h_reshaped: --> (batch_size, 1, hidden_size)\n",
    "        \n",
    "        encoder_states = encoder_states.permute(1, 2, 0)\n",
    "        # encoder_states: (batch_size, hidden_size, seq_length)\n",
    "        \n",
    "        energy = torch.bmm(h_reshaped, encoder_states)\n",
    "        # energy: (batch_size, 1, seq_length)\n",
    "        #print('raw attn, no scaling: ', energy[0])\n",
    "        energy = torch.mul(energy, scaling_factor)\n",
    "        #print('raw attn, with scaling: ', energy[0])\n",
    "        \n",
    "        attention = self.softmax(energy)\n",
    "        #print('attn: ', attention[0])\n",
    "        #s = torch.zeros(1).to(device)\n",
    "        \n",
    "        #for i in range(attention.shape[2]):\n",
    "        #    s += attention[0][0][i]\n",
    "        \n",
    "        #print('attn[-1]: ', attention[0][0][-1].float())\n",
    "        # attention: (N, 1, seq_length)\n",
    "        #print('sum: ', s)\n",
    "        \n",
    "        encoder_states = encoder_states.transpose(1,2)\n",
    "        # (N, hidden_size, seq_length) --> (N, seq_length, hidden_size)\n",
    "        \n",
    "        weighted_sum = torch.bmm(attention, encoder_states).permute(1,0,2)\n",
    "        # (N, 1, hidden_size) --> (1, N, hidden_size)\n",
    "        \n",
    "        predictions = self.fc(weighted_sum)\n",
    "        # shape of predictions: (1, N, 1)\n",
    "        #print('predictions:', format(predictions.shape))\n",
    "        \n",
    "        predictions = predictions.squeeze(0)\n",
    "        # (N, 1)\n",
    "        \n",
    "        predictions = self.sigmoid(predictions)\n",
    "        # nn.CrossEntropyLoss(LogSoftmax + NLLoss) doesn't need to do Softmax\n",
    "        \n",
    "        return predictions, hidden, cell, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "heard-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, source_warmup, source_test, scaling_factor):\n",
    "        # source(_warmup, _test): (seq_len, batch_size, input_size) ex) (30, N, 1)\n",
    "        # embedding_size = 128\n",
    "        # target: (target_len, batch_size, target_vocab_size) ex) (30, N, 1)\n",
    "        batch_size = source_warmup.shape[1]\n",
    "        target_len = source_warmup.shape[0]\n",
    "        # target_vocab_size = 1\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, 1).to(device)\n",
    "        \n",
    "        encoder_states, hidden, cell = self.encoder(source_warmup)\n",
    "        \n",
    "        \n",
    "        for t in range(target_len): \n",
    "            output, hidden, cell, attention = self.decoder(source_test[t], encoder_states, hidden, cell,\n",
    "                                               scaling_factor)\n",
    "            \n",
    "            #print(encoder_states.shape)\n",
    "            #print(source_test[t].shape)\n",
    "            #print(hidden.shape)\n",
    "            encoder_states = torch.cat((encoder_states, hidden), dim=0)\n",
    "            #print(encoder_states.shape)\n",
    "            \n",
    "            #print('tmp output', format(output.shape))\n",
    "            \n",
    "            # best_guess = torch.max(output, 1).values\n",
    "            #print(best_guess.shape)\n",
    "            #print(type(best_guess))\n",
    "            # best_guess: (N)\n",
    "            '''\n",
    "            for i in range(len(best_guess)):\n",
    "                if best_guess[i] > 0.5: # how about '0 -> rand(batch_size)' ?\n",
    "                    outputs[t] = 1\n",
    "                elif outputs[t] = 0\n",
    "            '''\n",
    "            outputs[t] = output\n",
    "            # outputs[t] = [1 if i > 0.5 else 0 for i in best_guess]\n",
    "            # outputs: (target_len, batch_size, target_vocab_size)\n",
    "            #print('tmp outputs:', format(outputs.shape))\n",
    "        \n",
    "        #print('total outputs:', format(outputs.shape))\n",
    "        return outputs, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "intended-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old - Non-overlapping trace sequence\n",
    "class TraceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, n, unique_pc_length, pc, y):\n",
    "        self.num_seq = n\n",
    "        self.pc = pc\n",
    "        self.y = y\n",
    "        self.unique_pc_length = unique_pc_length\n",
    "    \n",
    "    def get_y():\n",
    "        return self.y\n",
    "    \n",
    "    def get_pc():\n",
    "        return self.pc\n",
    "        \n",
    "    def __len__(self):\n",
    "        if len(self.pc[-1]) == 2*self.num_seq:\n",
    "            return len(self.pc)\n",
    "        else:\n",
    "            return len(self.pc)-1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        X = torch.fmod(self.pc[idx], self.unique_pc_length)\n",
    "        y = self.y[2*idx+1]\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new - Overlapping trace sequence\n",
    "class TraceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, n, unique_pc_length, pc, y):\n",
    "        self.num_seq = n\n",
    "        self.pc = pc\n",
    "        self.y = y\n",
    "        self.unique_pc_length = unique_pc_length\n",
    "    \n",
    "    def get_y():\n",
    "        return self.y\n",
    "    \n",
    "    def get_pc():\n",
    "        return self.pc\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pc) - 2*n + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        X = torch.fmod(self.pc[idx:idx+2*n], self.unique_pc_length)\n",
    "        y = self.y[idx+n:idx+2*n]\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "narrative-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPTgen:\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.num_hit = 0\n",
    "        self.num_miss = 0\n",
    "        self.demand_access = 0\n",
    "        self.prefetch_access = 0\n",
    "        self.CACHE_SIZE = size\n",
    "        self.liveness_history = [0 for i in range(128)]\n",
    "        \n",
    "        #self.addr_history = {}\n",
    "    \n",
    "    def add_access(self, curr_quanta):\n",
    "        self.demand_access += 1\n",
    "        (self.liveness_history)[curr_quanta] = 0\n",
    "    \n",
    "    def add_prefetch(self, curr_quanta):\n",
    "        self.prefetch_access = 0\n",
    "        (self.liveness_history)[curr_quanta] = 0\n",
    "        \n",
    "    def should_cache(self, curr_quanta, last_quanta):\n",
    "        is_cache = 1\n",
    "        i = last_quanta\n",
    "        while i != curr_quanta:\n",
    "            if self.liveness_history[i] >= self.CACHE_SIZE:\n",
    "                is_cache = 0\n",
    "                break\n",
    "            i = (i + 1) % len(self.liveness_history)\n",
    "        \n",
    "        if is_cache:\n",
    "            i = last_quanta;\n",
    "            while i != curr_quanta:\n",
    "                self.liveness_history[i] += 1;\n",
    "                i = (i+1) % len(self.liveness_history)\n",
    "        \n",
    "        if is_cache:\n",
    "            self.num_hit += 1\n",
    "        else:\n",
    "            self.num_miss += 1\n",
    "        \n",
    "        return is_cache\n",
    "    \n",
    "    def get_num_opt_hits():\n",
    "        return self.num_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "laughing-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_addr_history_lru(addr_history, curr_set, curr_lru):\n",
    "    for key, value in addr_history[curr_set].items():\n",
    "        if value['lru'] < curr_lru: \n",
    "            value['lru'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "homeless-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_lru_addr_history_element(addr_history, curr_set):\n",
    "    lru_addr = 0\n",
    "    for key, value in addr_history[curr_set].items():\n",
    "        timer = value['prev_time']\n",
    "        if value['lru'] == 128 -1:\n",
    "            lru_addr = key\n",
    "            break\n",
    "    del addr_history[curr_set][lru_addr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "casual-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeOPT(pc, mem_addr):\n",
    "    \n",
    "    perset_optgen = [ OPTgen(16) for i in range(2048)] \n",
    "    addr_history = [ {} for i in range(2048) ] #list of dictionary: 2048 set * 16 way\n",
    "    hit_miss_history = [ 0 for i in range(len(pc))]\n",
    "    perset_time = [ 0 for i in range(2048) ]\n",
    "    \n",
    "    for i in range(len(pc)): # put your sample set # of PCs\n",
    "        #print(bin(pc[i]))\n",
    "        curr_tag = (mem_addr[i] >> 17) & (2**(48 - 11 - 6) -1) # 6 -> bo+g, 11 -> idx \n",
    "        curr_set = (mem_addr[i] >> 6) & (2**11 - 1)\n",
    "        curr_time = perset_time[curr_set] % 128\n",
    "        \n",
    "        #print(bin(curr_tag))\n",
    "        if curr_tag in addr_history[curr_set]:\n",
    "            prev_time = addr_history[curr_set][curr_tag]['prev_time'] % 128\n",
    "            hit_miss_history[addr_history[curr_set][curr_tag]['idx']] = perset_optgen[curr_set].should_cache(curr_time, prev_time)\n",
    "            perset_optgen[curr_set].add_access(curr_time)\n",
    "            update_addr_history_lru(addr_history, curr_set, addr_history[curr_set][curr_tag]['lru'])\n",
    "        else:\n",
    "            if len(addr_history[curr_set]) >= 128:\n",
    "                #print(curr_set)\n",
    "                #print(addr_history[curr_set])\n",
    "                delete_lru_addr_history_element(addr_history, curr_set)\n",
    "            \n",
    "            addr_history[curr_set][curr_tag] = {}    \n",
    "            addr_history[curr_set][curr_tag]['lru'] = 0\n",
    "            perset_optgen[curr_set].add_access(curr_time)\n",
    "            update_addr_history_lru(addr_history,curr_set, 128 -1)\n",
    "            \n",
    "        perset_time[curr_set] += 1\n",
    "        addr_history[curr_set][curr_tag]['prev_time'] = curr_time\n",
    "        addr_history[curr_set][curr_tag]['idx'] = i\n",
    "        addr_history[curr_set][curr_tag]['lru'] = 0\n",
    "    \n",
    "    return hit_miss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "spanish-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trace(file, pc_seq, addr_seq):\n",
    "\n",
    "    pc = ''\n",
    "    addr = ''\n",
    "    f = open(file, 'r')\n",
    "    dic_pc = {}\n",
    "    dic_addr = {}\n",
    "    i = 0\n",
    "    while i < 49661289//4:\n",
    "        i += 1\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "        \n",
    "        line = line.replace('PC : ', '')\n",
    "        '''\n",
    "        if 'src_mem' in line:\n",
    "            line = line.replace(' src_mem[0]: ', '')\n",
    "        elif 'dest_mem' in line:\n",
    "            line = line.replace(' dest_mem[0]: ', '')\n",
    "        '''\n",
    "        \n",
    "        line = line.replace(' addr : ', '')\n",
    "        pc, addr = line.split(sep=',')\n",
    "        \n",
    "        dic_pc[pc] = 'yes'\n",
    "        dic_addr[addr] = 'yes'\n",
    "        pc_seq.append(int(pc))\n",
    "        addr_seq.append(int(addr))\n",
    "    \n",
    "    return len(dic_pc), len(dic_addr)\n",
    "    # pc_seq = torch.tensor(pc_seq)\n",
    "        \n",
    "    # total_pc = torch.split(pc_seq, 30)\n",
    "    \n",
    "    '''\n",
    "    train_pc = total_pc[:int(len(total_pc)*0.6)] # need to change as various trace files\n",
    "    val_pc = total_pc[int(len(total_pc)*0.6):int(len(total_pc)*0.8)+1]\n",
    "    test_pc = total_pc[int(len(total_pc)*0.8):]\n",
    "\n",
    "    train_send = []\n",
    "    val_send =[]\n",
    "    test_send = []\n",
    "\n",
    "    for i in train_pc:\n",
    "        train_send.append(i)\n",
    "    \n",
    "    for i in val_pc:\n",
    "        val_send.append(i)\n",
    "    \n",
    "    for i in test_pc:\n",
    "        test_send.append(i)\n",
    "    '''\n",
    "    \n",
    "    #return train_send, val_send, test_send\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "emotional-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old\n",
    "\n",
    "# hyperparameters\n",
    "file = \"../ChampSim/ChampSim/big_llc_trace/mcf_46B.txt\"\n",
    "# unique_pc_length = 2143 # need to change by programs!\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "n = 30\n",
    "\n",
    "\n",
    "total_pc = []\n",
    "total_addr = []\n",
    "\n",
    "train_pc = []\n",
    "val_pc = []\n",
    "test_pc = []\n",
    "\n",
    "\n",
    "unique_pc_length, unique_addr_length = extract_trace(file, total_pc, total_addr)\n",
    "\n",
    "y_true = []\n",
    "\n",
    "train_true = []\n",
    "val_true = []\n",
    "test_true = []\n",
    "\n",
    "y_true = makeOPT(total_pc, total_addr)\n",
    "\n",
    "train_pc = total_pc[:int(len(total_pc)*train_ratio)] # need to change as various trace files\n",
    "val_pc = total_pc[int(len(total_pc)*train_ratio):int(len(total_pc)*(train_ratio+val_ratio))+1] # +1 is not needed (just for match even #)\n",
    "test_pc = total_pc[int(len(total_pc)*(train_ratio+val_ratio)):]\n",
    "\n",
    "train_y = y_true[:int(len(y_true)*train_ratio)] # need to change as various trace files\n",
    "val_y = y_true[int(len(y_true)*train_ratio):int(len(y_true)*(train_ratio+val_ratio))+1] # +1 is not needed (just for match even #)\n",
    "test_y = y_true[int(len(y_true)*(train_ratio+val_ratio)):]\n",
    "\n",
    "train_pc = torch.tensor(train_pc)\n",
    "train_pc = torch.split(train_pc, 2*n)\n",
    "\n",
    "train_y = torch.tensor(train_y)\n",
    "train_y = torch.split(train_y, n)\n",
    "\n",
    "val_pc = torch.tensor(val_pc)\n",
    "val_pc = torch.split(val_pc, 2*n)\n",
    "        \n",
    "val_y = torch.tensor(val_y)\n",
    "val_y = torch.split(val_y, n)\n",
    "\n",
    "test_pc = torch.tensor(test_pc)\n",
    "test_pc = torch.split(test_pc, 2*n)\n",
    "        \n",
    "test_y = torch.tensor(test_y)\n",
    "test_y = torch.split(test_y, n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "\n",
    "# hyperparameters\n",
    "file = \"../ChampSim/ChampSim/big_llc_trace/mcf_46B.txt\"\n",
    "# unique_pc_length = 2143 # need to change by programs!\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "n = 30\n",
    "\n",
    "\n",
    "total_pc = []\n",
    "total_addr = []\n",
    "\n",
    "train_pc = []\n",
    "val_pc = []\n",
    "test_pc = []\n",
    "\n",
    "\n",
    "unique_pc_length, unique_addr_length = extract_trace(file, total_pc, total_addr)\n",
    "\n",
    "y_true = []\n",
    "\n",
    "train_true = []\n",
    "val_true = []\n",
    "test_true = []\n",
    "\n",
    "y_true = makeOPT(total_pc, total_addr)\n",
    "\n",
    "train_pc = total_pc[:int(len(total_pc)*train_ratio)] # need to change as various trace files\n",
    "val_pc = total_pc[int(len(total_pc)*train_ratio):int(len(total_pc)*(train_ratio+val_ratio))+1] # +1 is not needed (just for match even #)\n",
    "test_pc = total_pc[int(len(total_pc)*(train_ratio+val_ratio)):]\n",
    "\n",
    "train_y = y_true[:int(len(y_true)*train_ratio)] # need to change as various trace files\n",
    "val_y = y_true[int(len(y_true)*train_ratio):int(len(y_true)*(train_ratio+val_ratio))+1] # +1 is not needed (just for match even #)\n",
    "test_y = y_true[int(len(y_true)*(train_ratio+val_ratio)):]\n",
    "\n",
    "train_pc = torch.tensor(train_pc)\n",
    "\n",
    "train_y = torch.tensor(train_y)\n",
    "\n",
    "val_pc = torch.tensor(val_pc)\n",
    "        \n",
    "val_y = torch.tensor(val_y)\n",
    "\n",
    "test_pc = torch.tensor(test_pc)\n",
    "        \n",
    "test_y = torch.tensor(test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "inappropriate-pathology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTgen hit rate: 63.108206134323375\n",
      "12415322\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i]:\n",
    "        correct += 1\n",
    "print('OPTgen hit rate:', correct / len(y_true) * 100)\n",
    "print(len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "stainless-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TraceDataset(n, unique_pc_length, train_pc, train_y)\n",
    "valset = TraceDataset(n, unique_pc_length, val_pc, val_y)\n",
    "testset = TraceDataset(n, unique_pc_length, test_pc, test_y)\n",
    "\n",
    "partition = {'train': trainset, 'val': valset, 'test' : testset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "rubber-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "#attn_train = torch.zeros(batch_size, 1, 2*n)\n",
    "#attn_val = torch.zeros(batch_size, 1, 2*n)\n",
    "#attn_test = torch.zeros(batch_size, 1, 2*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "important-relationship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "2243467\n",
      "13353.970238095239\n",
      "41385\n",
      "82769\n",
      "124154\n",
      "124153\n",
      "13\n",
      "pc length 12415322\n"
     ]
    }
   ],
   "source": [
    "print(unique_pc_length)\n",
    "print(unique_addr_length)\n",
    "print(unique_addr_length/unique_pc_length)\n",
    "print(len(val_pc))\n",
    "print(len(val_y))\n",
    "print(len(train_pc))\n",
    "print(len(trainset))\n",
    "print(len(train_pc[-1]))\n",
    "print('pc length', format(len(total_pc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "shaped-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 1\n",
    "learning_rate = 0.0001\n",
    "l2 = 0.00001\n",
    "batch_size = 128\n",
    "\n",
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "input_size_encoder = unique_pc_length # need to change as various program! (2143=600.perlbench)\n",
    "input_size_decoder = unique_pc_length\n",
    "output_size = 10\n",
    "encoder_embedding_size = 128\n",
    "decoder_embedding_size = 128\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "enc_dropout = 0.0\n",
    "dec_dropout = 0.0\n",
    "\n",
    "scaling_factor = 5\n",
    "\n",
    "optimizer = 'Adam'\n",
    "\n",
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size,\n",
    "                     num_layers, enc_dropout).to(device)\n",
    "\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size,\n",
    "                     output_size, num_layers, dec_dropout).to(device)\n",
    "\n",
    "model = AttnDecoder(encoder_net, decoder_net).to(device) # *******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "virtual-auditor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/junseo/cache-replacement\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "married-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, input_data, optimizer, loss_fn):\n",
    "    trainloader = DataLoader(partition['train'],\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=True, drop_last=True)\n",
    "    \n",
    "    #print('train', format(len(trainloader)))\n",
    "    \n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    for i, (X ,y) in enumerate(trainloader):\n",
    "        #print(i)\n",
    "        \n",
    "        # X: (batch_size, 2*seq_length) --> (2*seq_length, batch_size)\n",
    "        X_warmup = X[:,0:30].transpose(0,1).to(device)\n",
    "        X_test = X[:, 30:].transpose(0,1).to(device)\n",
    "        y_true = y.to(device) # y: (batch_size, seq_length)\n",
    "        \n",
    "        # (seq_length, batch_size) --> (2*seq_length, batch_size, 1)\n",
    "        #X_warmup = X_warmup.unsqueeze(-1)\n",
    "        #X_test = X_test.unsqueeze(-1)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        y_pred, attn_train = model(X_warmup, X_test, scaling_factor)\n",
    "        \n",
    "        \n",
    "        y_pred = y_pred.transpose(0,1).squeeze(2)\n",
    "        # y_pred: (seq_length, batch_size, 1) --> (batch_size, seq_length)\n",
    "        # y_true: (batch_size, seq_length)\n",
    "        \n",
    "        #print(y_pred)\n",
    "        #print(y_true)\n",
    "        \n",
    "        #print('y_pred:', format(y_pred.shape))\n",
    "        #print('y_true:', format(y_true.shape))\n",
    "        loss = loss_fn(y_pred.reshape(-1).float(), y_true.reshape(-1).float())\n",
    "        #print('y_pred.view:', format(y_pred.reshape(-1).shape))\n",
    "        #print('y_true.view:', format(y_true.reshape(-1).shape))\n",
    "        #print(loss)\n",
    "        y_pred = y_pred.reshape(-1)\n",
    "        y_true = y_true.reshape(-1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += accuracy(y_pred, y_true)\n",
    "\n",
    "        \n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = train_acc / len(trainloader)\n",
    "    print('train_loss:', format(train_loss))\n",
    "    print('train_acc:', format(train_acc))\n",
    "    return model, train_loss, train_acc, attn_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "dutch-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, partition, loss_fn):\n",
    "    valloader = DataLoader(partition['val'],\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=True, drop_last=True)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(valloader):\n",
    "\n",
    "            # X: (batch_size, 2*seq_length) --> (2*seq_length, batch_size)\n",
    "            X_warmup = X[:,0:30].transpose(0,1).to(device)\n",
    "            X_test = X[:, 30:].transpose(0,1).to(device)\n",
    "            y_true = y.to(device) # y: (batch_size, seq_length)\n",
    "        \n",
    "            # (seq_length, batch_size) --> (2*seq_length, batch_size, 1)\n",
    "            #X_warmup = X_warmup.unsqueeze(-1)\n",
    "            #X_test = X_test.unsqueeze(-1)\n",
    "            \n",
    "            y_pred, attn_val = model(X_warmup, X_test, scaling_factor)\n",
    "            y_pred = y_pred.transpose(0,1).squeeze(2)\n",
    "            loss = loss_fn(y_pred.reshape(-1).float(), y_true.reshape(-1).float())\n",
    "            \n",
    "            y_pred = y_pred.reshape(-1)\n",
    "            y_true = y_true.reshape(-1)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_acc += accuracy(y_pred, y_true)\n",
    "\n",
    "    #print(len(valloader))\n",
    "    val_loss = val_loss / len(valloader)\n",
    "    val_acc = val_acc / len(valloader)\n",
    "    print('val_loss:', format(val_loss))\n",
    "    print('val_acc:', format(val_acc))\n",
    "    \n",
    "    return val_loss, val_acc, attn_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "consecutive-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, partition):\n",
    "    testloader = DataLoader(partition['test'], \n",
    "                           batch_size=batch_size, \n",
    "                           shuffle=True, drop_last=True)\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    test_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(testloader):\n",
    "\n",
    "            # X: (batch_size, 2*seq_length) --> (2*seq_length, batch_size)\n",
    "            X_warmup = X[:,0:30].transpose(0,1).to(device)\n",
    "            X_test = X[:, 30:].transpose(0,1).to(device)\n",
    "            y_true = y.to(device) # y: (batch_size, seq_length)\n",
    "        \n",
    "            # (seq_length, batch_size) --> (2*seq_length, batch_size, 1)\n",
    "            #X_warmup = X_warmup.unsqueeze(-1)\n",
    "            #X_test = X_test.unsqueeze(-1)\n",
    "            \n",
    "            \n",
    "            y_pred, attn_test = model(X_warmup, X_test, scaling_factor)\n",
    "            y_pred = y_pred.transpose(0,1).squeeze(2)\n",
    "            \n",
    "            y_pred = y_pred.reshape(-1)\n",
    "            y_true = y_true.reshape(-1)\n",
    "            \n",
    "            test_acc += accuracy(y_pred, y_true)\n",
    "\n",
    "    test_acc = test_acc / len(testloader)\n",
    "    print('test_acc:' ,format(test_acc))\n",
    "    return test_acc, attn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "terminal-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition):\n",
    "    \n",
    "    encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size,\n",
    "                     num_layers, enc_dropout).to(device)\n",
    "\n",
    "    decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size,\n",
    "                     output_size, num_layers, dec_dropout).to(device)\n",
    "\n",
    "    model = AttnDecoder(encoder_net, decoder_net).to(device) # *******\n",
    "    \n",
    "    loss_fn = nn.BCELoss()\n",
    "    \n",
    "    if optimizer == 'SGD':\n",
    "        optimize = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=l2)\n",
    "    elif optim == 'RMSprop':\n",
    "        optimize = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=l2)\n",
    "    elif optimizer == 'Adam':\n",
    "        optimize = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2)\n",
    "    else:\n",
    "        raise ValueError('in-valid optimizer choice')\n",
    "        \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    train_attns = []\n",
    "    val_attns = []\n",
    "    \n",
    "    result = {}\n",
    "    for epoch in range(num_epochs):\n",
    "        ts = time.time()\n",
    "        model, train_loss, train_acc, attn_train = train(model, partition, optimize, loss_fn)\n",
    "        val_loss, val_acc, attn_val = validate(model, partition, loss_fn)\n",
    "        te = time.time()\n",
    "        train_attns.append(attn_train)\n",
    "        val_attns.append(attn_val)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print('Epoch {}, ACC(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "        \n",
    "        \n",
    "    test_acc, attn_test = test(model, partition)\n",
    "    result['test_attn'] = attn_test\n",
    "    \n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_attns'] = train_attns\n",
    "    result['val_attns'] = val_attns\n",
    "    #result['train_acc'] = train_acc\n",
    "    #result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "    return result, attn_train, attn_val, attn_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "received-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    # y_pred: (N, seq_length, 2)\n",
    "    # y_true: (N, seq_length)\n",
    "    y = [1 if i>0.5 else 0 for i in y_pred]\n",
    "    correct = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == y_true[i]:\n",
    "            correct += 1\n",
    "    return correct/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "persistent-thong",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"        \\ndef load_exp_result(exp_name):\\n    dir_path = './results'\\n    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\\n    list_result = []\\n    for filename in filenames:\\n        if exp_name in filename:\\n            with open(join(dir_path, filename), 'r') as infile:\\n                results = json.load(infile)\\n                list_result.append(results)\\n    df = pd.DataFrame(list_result)\\n    return df\\n\""
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "\n",
    "#!mkdir results\n",
    "'''\n",
    "def save_exp_result(result):\n",
    "    exp_name = 'exp_1'\n",
    "    # del setting['epoch']\n",
    "    \n",
    "    # hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
    "    filename = './results/{}-{}.json'.format(exp_name, 1)\n",
    "    # result.update(setting) ????\n",
    "    with(filename, 'w') as f:\n",
    "        json.dump(result, f)\n",
    "'''\n",
    "'''        \n",
    "def load_exp_result(exp_name):\n",
    "    dir_path = './results'\n",
    "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
    "    list_result = []\n",
    "    for filename in filenames:\n",
    "        if exp_name in filename:\n",
    "            with open(join(dir_path, filename), 'r') as infile:\n",
    "                results = json.load(infile)\n",
    "                list_result.append(results)\n",
    "    df = pd.DataFrame(list_result)\n",
    "    return df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "accurate-dylan",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.6354837332716667\n",
      "train_acc: 0.6416997226522201\n",
      "val_loss: 0.6460896189736877\n",
      "val_acc: 0.6250137061403513\n",
      "Epoch 0, ACC(train/val): 0.64/0.63, Loss(train/val) 0.63548/0.64609. Took 19756.23 sec\n",
      "test_acc: 0.6047552244582043\n",
      "OPTgen hit rate: 63.108206134323375\n"
     ]
    }
   ],
   "source": [
    "result, attn_train, attn_val, attn_test = experiment(partition)\n",
    "#save_exp_result(result)\n",
    "correct = 0\n",
    "for i in range(len(y_true)):\n",
    "    if y_true[i]:\n",
    "        correct += 1\n",
    "print('OPTgen hit rate:', correct / len(y_true) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x, y, attn):\n",
    "    z = np.zeros((len(y),len(x)))\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(y)):\n",
    "            z[j][i] = attn[i][0][j]\n",
    "    \n",
    "    return z\n",
    "\n",
    "x = np.linspace(0, 63, 64)\n",
    "y = np.linspace(0, 58, 59)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "attn_train_np = attn_train.cpu().detach().numpy()\n",
    "\n",
    "Z = f(x, y, attn_train_np)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Z.shape)\n",
    "print(attn_train_np.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('sequence')\n",
    "ax.set_zlabel('attn_train');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = attn_train.squeeze(1).cpu().detach().numpy()\n",
    "\n",
    "print(z.shape)\n",
    "plt.figure(figsize = (5.5, 5.5))\n",
    "plt.gray()\n",
    "plt.pcolor(z)\n",
    "plt.colorbar()\n",
    "plt.xlabel('sequence')\n",
    "plt.ylabel('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_val_np = attn_val.cpu().detach().numpy()\n",
    "\n",
    "Z = f(x, y, attn_val_np)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Z.shape)\n",
    "print(attn_val_np.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('sequence')\n",
    "ax.set_zlabel('attn_val');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = attn_val.squeeze(1).cpu().detach().numpy()\n",
    "\n",
    "print(z.shape)\n",
    "plt.figure(figsize = (5.5, 5.5))\n",
    "plt.gray()\n",
    "plt.pcolor(z)\n",
    "plt.colorbar()\n",
    "plt.xlabel('sequence')\n",
    "plt.ylabel('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_test_np = attn_test.cpu().detach().numpy()\n",
    "\n",
    "Z = f(x, y, attn_test_np)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Z.shape)\n",
    "print(attn_test_np.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X, Y, Z, 50, cmap='binary')\n",
    "ax.set_xlabel('batch')\n",
    "ax.set_ylabel('sequence')\n",
    "ax.set_zlabel('attn_test');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = attn_test.squeeze(1).cpu().detach().numpy()\n",
    "\n",
    "print(z.shape)\n",
    "plt.figure(figsize = (5.5, 5.5))\n",
    "plt.gray()\n",
    "plt.pcolor(z)\n",
    "plt.colorbar()\n",
    "plt.xlabel('sequence')\n",
    "plt.ylabel('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-keeping",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
